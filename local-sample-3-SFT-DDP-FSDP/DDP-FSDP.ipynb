{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -q -U transformers datasets bitsandbytes  trl peft  huggingface_hub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom trl import SFTTrainer, SFTConfig\nfrom torch.optim import AdamW\nfrom transformers import get_scheduler\nfrom accelerate import PartialState\nimport os\n\nfrom math import ceil\n\n\n# local_save_path_model = \"New-model-full-ddp\"\n# WANDB_API_KEY = \"xxxxxxxxxxxx\"\n# os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### DDP","metadata":{}},{"cell_type":"code","source":"df = pd.read_parquet(\"hf://datasets/ayoubkirouane/arxiv-physics/data/train-00000-of-00001-5bba4a271402bdbb.parquet\")\ntrain_dataset = Dataset.from_pandas(df).select(range(1000))\ntrain_dataset = train_dataset.add_column(\n    \"messages\",\n    [[{'content': row['question'], 'role': 'user'}, {'content': row['answer'], 'role': 'assistant'}] for row in train_dataset]\n)\n\ndevice_string = PartialState().process_index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_model_and_tokenizer(model_name, use_gpu = True):\n\n    # Load base model and tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name )\n    model = AutoModelForCausalLM.from_pretrained(model_name , device_map={'':device_string})\n    if not tokenizer.chat_template:\n        tokenizer.chat_template = \"\"\"{% for message in messages %}\n                {% if message['role'] == 'system' %}System: {{ message['content'] }}\\n\n                {% elif message['role'] == 'user' %}User: {{ message['content'] }}\\n\n                {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }} <|endoftext|>\n                {% endif %}\n                {% endfor %}\"\"\"\n\n    # Tokenizer config\n    if not tokenizer.pad_token:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    return model, tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, tokenizer = load_model_and_tokenizer(\"unsloth/Qwen3-1.7B\", True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sft_config = SFTConfig(\n    learning_rate=8e-5, # Learning rate for training.\n    num_train_epochs=1, #  Set the number of epochs to train the model.\n    per_device_train_batch_size=1, # Batch size for each device (e.g., GPU) during training.\n    gradient_accumulation_steps=8, # Number of steps before performing a backward/update pass to accumulate gradients.\n    gradient_checkpointing=True, # Enable gradient checkpointing to reduce memory usage during training at the cost of slower training speed.\n    gradient_checkpointing_kwargs = {\"use_reentrant\": False}, # Must be false for DDP\n    logging_steps=1,  # Frequency of logging training progress (log every 2 steps).\n    dataset_text_field=\"messages\",\n    report_to=\"wandb\"\n\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_update_steps_per_epoch = ceil(len(train_dataset) / sft_config.per_device_train_batch_size / sft_config.gradient_accumulation_steps)\ntotal_training_steps = num_update_steps_per_epoch * sft_config.num_train_epochs\n\n# Create optimizer\noptimizer = AdamW(model.parameters(), lr=sft_config.learning_rate)\n\n# Create learning rate scheduler\nlr_scheduler = get_scheduler(\n    name=\"cosine\",  # or \"cosine\", \"constant\",\"linear\" etc.\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_training_steps\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sft_trainer = SFTTrainer(\n    model=model,\n    args=sft_config,\n    train_dataset=train_dataset,\n    processing_class=tokenizer,\n    optimizers=(optimizer, lr_scheduler)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sft_trainer.train()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-22T18:34:17.363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sft_trainer.model.save_pretrained(\"New_Full-Qwen3-1.7B\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-22T18:34:17.363Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### FSDP","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom trl import SFTTrainer, SFTConfig\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LOCAL_SAVE_PATH = \"New-model-full-finetune-fsdp\"\n\n# Load dataset\nWANDB_API_KEY = \"xxxxxxxxxxx\"\nos.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_parquet(\"hf://datasets/ayoubkirouane/arxiv-physics/data/train-00000-of-00001-5bba4a271402bdbb.parquet\")\ntrain_dataset = Dataset.from_pandas(df).select(range(1000))\ntrain_dataset = train_dataset.add_column(\n    \"messages\",\n    [[{'content': row['question'], 'role': 'user'}, {'content': row['answer'], 'role': 'assistant'}] for row in train_dataset]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model & tokenizer\ndef load_model_and_tokenizer(model_name):\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n\n    if not tokenizer.chat_template:\n        tokenizer.chat_template = \"\"\"{% for message in messages %}\n        {% if message['role'] == 'system' %}System: {{ message['content'] }}\\n\n        {% elif message['role'] == 'user' %}User: {{ message['content'] }}\\n\n        {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }} <|endoftext|>\n        {% endif %}\n        {% endfor %}\"\"\"\n\n    tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n    return model, tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"unsloth/llama-3.2-3b-instruct\"\nmodel, tokenizer = load_model_and_tokenizer(model_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sft_config = SFTConfig(\n    output_dir=LOCAL_SAVE_PATH,\n    dataset_text_field=\"messages\",\n    max_seq_length=2048,\n    learning_rate=8e-5,\n    num_train_epochs=1,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    gradient_checkpointing=True,\n    logging_steps=1,\n    save_strategy=\"epoch\",\n    report_to=\"wandb\",\n    fsdp=\"full_shard auto_wrap\",\n    fsdp_config={\n        \"fsdp_transformer_layer_cls_to_wrap\": \"LlamaDecoderLayer\"    },\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    args=sft_config,\n    train_dataset=train_dataset,\n    processing_class=tokenizer,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.save_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fsdp_config.yaml\n# compute_environment: LOCAL_MACHINE\n# debug: false\n# distributed_type: FSDP\n# downcast_bf16: 'no'\n# fsdp_config:\n#   fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n#   fsdp_backward_prefetch: BACKWARD_PRE\n#   fsdp_cpu_ram_efficient_loading: true\n#   fsdp_forward_prefetch: false\n#   fsdp_offload_params: false\n#   fsdp_sharding_strategy: FULL_SHARD\n#   fsdp_state_dict_type: SHARDED_STATE_DICT\n#   fsdp_sync_module_states: true\n#   fsdp_use_orig_params: false\n# machine_rank: 0\n# main_training_function: main\n# mixed_precision: bf16\n# num_machines: 1\n# num_processes: 2\n# rdzv_backend: static\n# same_network: true\n# tpu_env: []\n# tpu_use_cluster: false\n# tpu_use_sudo: false\n# use_cpu: false\n\n# accelerate launch --config_file fsdp_config.yaml train.py","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}