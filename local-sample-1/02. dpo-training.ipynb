{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-22T01:00:18.239072Z",
     "iopub.status.busy": "2025-07-22T01:00:18.238832Z",
     "iopub.status.idle": "2025-07-22T01:03:36.941234Z",
     "shell.execute_reply": "2025-07-22T01:03:36.940463Z",
     "shell.execute_reply.started": "2025-07-22T01:00:18.239046Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install trl unsloth comet-ml>=3.43.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T01:03:36.943444Z",
     "iopub.status.busy": "2025-07-22T01:03:36.943205Z",
     "iopub.status.idle": "2025-07-22T01:04:11.961960Z",
     "shell.execute_reply": "2025-07-22T01:04:11.961139Z",
     "shell.execute_reply.started": "2025-07-22T01:03:36.943418Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 01:03:45.227151: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753146225.430712      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753146225.493254      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "comet_ml is installed but the Comet API Key is not configured. Please set the `COMET_API_KEY` environment variable to enable Comet logging. Check out the documentation for other ways of configuring it: https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#set-the-api-key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import PatchDPOTrainer\n",
    "\n",
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T01:04:11.962952Z",
     "iopub.status.busy": "2025-07-22T01:04:11.962690Z",
     "iopub.status.idle": "2025-07-22T01:04:11.971056Z",
     "shell.execute_reply": "2025-07-22T01:04:11.970411Z",
     "shell.execute_reply.started": "2025-07-22T01:04:11.962928Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from trl import DPOConfig, DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T01:04:11.971866Z",
     "iopub.status.busy": "2025-07-22T01:04:11.971642Z",
     "iopub.status.idle": "2025-07-22T01:04:40.380805Z",
     "shell.execute_reply": "2025-07-22T01:04:40.380207Z",
     "shell.execute_reply.started": "2025-07-22T01:04:11.971851Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.6: Fast Llama patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 6.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a04c9a457445edaaf2ff0d05c304e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c61504dce94e09b74d53f2e9d22dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3510784b1d464804941538b737440855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fadb721d43d45409f24f01fc77d8324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74965b280f5c4cf5b47e556127b2431d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10199f4b56b41778ad37501489b2ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23ceb4a37a24e519f5317c8a5c36030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1cd6f6d968c472d8d156220927317cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e34d813a9f4412adaede060703dd6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_seq_length = 2048\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"SkillRipper/TwinLlama-3.2-3B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T01:04:40.382637Z",
     "iopub.status.busy": "2025-07-22T01:04:40.382404Z",
     "iopub.status.idle": "2025-07-22T01:04:47.547869Z",
     "shell.execute_reply": "2025-07-22T01:04:47.547262Z",
     "shell.execute_reply.started": "2025-07-22T01:04:40.382619Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.7.6 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T01:04:47.548861Z",
     "iopub.status.busy": "2025-07-22T01:04:47.548598Z",
     "iopub.status.idle": "2025-07-22T01:04:49.912505Z",
     "shell.execute_reply": "2025-07-22T01:04:49.911812Z",
     "shell.execute_reply.started": "2025-07-22T01:04:47.548827Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fad71d52b44b87a1e911e15d22526e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/369 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21045b73f263464581d969404b46b7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/242k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c192329e774a95a6efd723425db1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " dataset = load_dataset(\"SkillRipper/preference-data\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T01:04:49.913736Z",
     "iopub.status.busy": "2025-07-22T01:04:49.913427Z",
     "iopub.status.idle": "2025-07-22T01:04:50.025850Z",
     "shell.execute_reply": "2025-07-22T01:04:50.025112Z",
     "shell.execute_reply.started": "2025-07-22T01:04:49.913705Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9d71ca4fd943f780dc5837b37572e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpaca_template = \"\"\"\n",
    "Below is an instruction that describes a task. \n",
    "Write a response that appropriately completes the request.\n",
    " ### Instruction:\n",
    " {}\n",
    " ### Response:\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def format_samples(example):\n",
    "    example[\"prompt\"] = alpaca_template.format(example[\"prompt\"])\n",
    "    example[\"chosen\"] = example['chosen'] + EOS_TOKEN\n",
    "    example[\"rejected\"] = example['rejected'] + EOS_TOKEN\n",
    "    \n",
    "    return {\"prompt\": example[\"prompt\"], \"chosen\": \n",
    "\n",
    "example[\"chosen\"], \"rejected\": example[\"rejected\"]}\n",
    "dataset = dataset.map(format_samples)\n",
    "dataset = dataset.train_test_split(test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T01:04:50.026925Z",
     "iopub.status.busy": "2025-07-22T01:04:50.026642Z",
     "iopub.status.idle": "2025-07-22T01:21:01.849931Z",
     "shell.execute_reply": "2025-07-22T01:21:01.849236Z",
     "shell.execute_reply.started": "2025-07-22T01:04:50.026905Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ebabc0aba9a4f788aebb266c0411cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset (num_proc=2):   0%|          | 0/1054 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45b5a8146fe4ac3adbeb3bf27c9de14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=2):   0%|          | 0/1054 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2903a6c7ddcb4738bd9ca246602ebfc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/1054 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01484a7270ee456691c634377d49b5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in eval dataset (num_proc=2):   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d332e9e0be774123ad299b67f21653d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset (num_proc=2):   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e396ca086b714b13b4f2a5705eb23a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=2):   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,054 | Num Epochs = 1 | Total steps = 66\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 15:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>rewards / chosen</th>\n",
       "      <th>rewards / rejected</th>\n",
       "      <th>rewards / accuracies</th>\n",
       "      <th>rewards / margins</th>\n",
       "      <th>logps / chosen</th>\n",
       "      <th>logps / rejected</th>\n",
       "      <th>logits / chosen</th>\n",
       "      <th>logits / rejected</th>\n",
       "      <th>eval_logits / chosen</th>\n",
       "      <th>eval_logits / rejected</th>\n",
       "      <th>nll_loss</th>\n",
       "      <th>aux_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.692200</td>\n",
       "      <td>0.690605</td>\n",
       "      <td>-0.001360</td>\n",
       "      <td>-0.006460</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>-138.949600</td>\n",
       "      <td>-82.829979</td>\n",
       "      <td>-0.321397</td>\n",
       "      <td>-0.091277</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.686600</td>\n",
       "      <td>0.682729</td>\n",
       "      <td>-0.006674</td>\n",
       "      <td>-0.027737</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.021062</td>\n",
       "      <td>-139.002716</td>\n",
       "      <td>-83.042732</td>\n",
       "      <td>-0.323317</td>\n",
       "      <td>-0.093832</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.677400</td>\n",
       "      <td>0.674293</td>\n",
       "      <td>-0.013561</td>\n",
       "      <td>-0.052001</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.038440</td>\n",
       "      <td>-139.071594</td>\n",
       "      <td>-83.285385</td>\n",
       "      <td>-0.325163</td>\n",
       "      <td>-0.095991</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.669017</td>\n",
       "      <td>-0.019273</td>\n",
       "      <td>-0.068679</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.049406</td>\n",
       "      <td>-139.128708</td>\n",
       "      <td>-83.452164</td>\n",
       "      <td>-0.326521</td>\n",
       "      <td>-0.097750</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but the Comet API Key is not configured. Please set the `COMET_API_KEY` environment variable to enable Comet logging. Check out the documentation for other ways of configuring it: https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#set-the-api-key\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=66, training_loss=0.6810455114552469, metrics={'train_runtime': 960.0786, 'train_samples_per_second': 1.098, 'train_steps_per_second': 0.069, 'total_flos': 0.0, 'train_loss': 0.6810455114552469, 'epoch': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    tokenizer=tokenizer,\n",
    "    beta=0.5,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    max_length=max_seq_length//2,\n",
    "    max_prompt_length=max_seq_length//2,\n",
    "    args=DPOConfig(\n",
    "        learning_rate=2e-6,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        num_train_epochs=1,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=10,\n",
    "        output_dir=\"output\",\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=0.2,\n",
    "        logging_steps=1,\n",
    "        report_to=\"none\",\n",
    "        seed=0,\n",
    "    ),\n",
    " )\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T01:21:01.851045Z",
     "iopub.status.busy": "2025-07-22T01:21:01.850798Z",
     "iopub.status.idle": "2025-07-22T01:21:06.177685Z",
     "shell.execute_reply": "2025-07-22T01:21:06.176887Z",
     "shell.execute_reply.started": "2025-07-22T01:21:01.851023Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>\n",
      "Below is an instruction that describes a task. \n",
      "Write a response that appropriately completes the request.\n",
      " ### Instruction:\n",
      " Write a paragraph to introduce supervised fine-tuning.\n",
      " ### Response:\n",
      " Supervised fine-tuning is a method used to adapt a pre-trained model to a specific task. In this process, the pre-trained model is trained on a labeled dataset to learn the task-specific patterns and relationships. The model learns to recognize and classify the task-specific features, and the trained model is then used for prediction or generation tasks. Supervised fine-tuning is a widely used technique in deep learning, especially for natural language processing tasks.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "message = alpaca_template.format(\"Write a paragraph to introduce supervised fine-tuning.\", \"\")\n",
    "inputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=256, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T01:21:06.178757Z",
     "iopub.status.busy": "2025-07-22T01:21:06.178504Z",
     "iopub.status.idle": "2025-07-22T01:21:57.182683Z",
     "shell.execute_reply": "2025-07-22T01:21:57.181968Z",
     "shell.execute_reply.started": "2025-07-22T01:21:06.178738Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Successfully copied all 2 files from cache to model.\n",
      "Downloading safetensors index for SkillRipper/TwinLlama-3.2-3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:38<00:00, 19.29s/it]\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T01:21:57.184632Z",
     "iopub.status.busy": "2025-07-22T01:21:57.184403Z",
     "iopub.status.idle": "2025-07-22T01:21:57.302263Z",
     "shell.execute_reply": "2025-07-22T01:21:57.301608Z",
     "shell.execute_reply.started": "2025-07-22T01:21:57.184614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T01:21:57.303566Z",
     "iopub.status.busy": "2025-07-22T01:21:57.303291Z",
     "iopub.status.idle": "2025-07-22T01:24:00.362805Z",
     "shell.execute_reply": "2025-07-22T01:24:00.362014Z",
     "shell.execute_reply.started": "2025-07-22T01:21:57.303542Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a214c41b20247db958aabb284946b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e565a0a8d74707970d59bea703a336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Successfully copied all 2 files from cache to SkillRipper/TwinLlama-3.2-3B-DPO.\n",
      "Downloading safetensors index for SkillRipper/TwinLlama-3.2-3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b1db1d27be4e61a7ae0548c6d25ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96881985f1447fe873178f071d94299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [01:08<01:08, 68.83s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7907e8651baa4c22a5a0b319aafc2fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a243e3205474a72929676afea531827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:29<00:00, 44.95s/it]\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub_merged(\"SkillRipper/TwinLlama-3.2-3B-DPO\", tokenizer, save_method=\"merged_16bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
